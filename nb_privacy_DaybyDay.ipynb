{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from matplotlib.dates import datestr2num\n",
    "# import torch.optim as optim\n",
    "# from sklearn import preprocessing\n",
    "# import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'daytabase_no_shuffle.csv'\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_SPLIT = 0.95\n",
    "\n",
    "NUM_POINTS_PER_ENTRY = 150\n",
    "NUM_FEATURES = 3*NUM_POINTS_PER_ENTRY\n",
    "NUM_UNITS = 512\n",
    "NUM_USERS = 9\n",
    "NUM_EPOCHS = 30\n",
    "\n",
    "EPSILON = 0.1\n",
    "DELTA = 10**-5\n",
    "NORM_CLIP=10\n",
    "\n",
    "SIGMA = 0.75\n",
    "\n",
    "RHO = 0.5\n",
    "\n",
    "UTILITY_WEIGHTS = (0,1,1,1)\n",
    "PRIVACY_WEIGHTS =(2,1)\n",
    "\n",
    "MAP_PARAMS = 2\n",
    "\n",
    "NUM_GRIDS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChaniaDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None, normalize=True):\n",
    "#         \n",
    "        self.augmented_data = pd.read_csv(csv_file, usecols = [0]+list(range(2,NUM_FEATURES+1)))\n",
    "        self.userlabels = pd.read_csv(csv_file, usecols=[0])\n",
    "        self.transform = transform\n",
    "        \n",
    "        if normalize:\n",
    "            self.augmented_data=(self.augmented_data-self.augmented_data.mean())/self.augmented_data.std()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.augmented_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if type(idx) == torch.Tensor:\n",
    "            idx = idx.item()\n",
    "        data = self.augmented_data.iloc[idx].values\n",
    "        data = data.astype('float').reshape(-1,NUM_FEATURES)            \n",
    "        user = self.userlabels.iloc[idx].values\n",
    "        user = user.astype('int').reshape(-1,1)\n",
    "        sample = {'x':data, 'u':user}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    def __call__(self, sample):\n",
    "        data, user = sample['x'], sample['u']\n",
    "        return {'x':torch.from_numpy(data), 'u':torch.from_numpy(user)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "chania_dataset = ChaniaDataset(csv_file=FILENAME, transform=ToTensor(), normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size=int(TRAIN_SPLIT*len(chania_dataset))\n",
    "test_size = len(chania_dataset)-train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = torch.utils.data.random_split(chania_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_privatizer(x, y, u, uhat):\n",
    "    with torch.no_grad():\n",
    "        fig, ax = plt.subplots(1,3,figsize=(15,5))\n",
    "        #sharex=True,sharey=True,\n",
    "        ax[0].set_title(\"Input Data\")\n",
    "        ax[1].set_title(\"Obfuscated Data\")\n",
    "        ax[2].set_title(\"Adversary Estimate\")\n",
    "        _, upred = torch.max(uhat.data,1)\n",
    "        for i in range(NUM_POINTS_PER_ENTRY):\n",
    "            ax[0].scatter(x[:,:,2+3*i].numpy(),x[:,:,1+3*i].numpy(),c=u.unsqueeze(0).numpy().T.tolist())\n",
    "            ax[1].scatter(y[:,:,2+3*i].numpy(),y[:,:,1+3*i].numpy(),c='black')\n",
    "            ax[2].scatter(y[:,:,2+3*i].numpy(),y[:,:,1+3*i].numpy(),c=upred.numpy().reshape(-1,1).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_maps(x, y,npoints=100):\n",
    "    with torch.no_grad():\n",
    "        fig, ax = plt.subplots(1,2,sharex=True,sharey=True,figsize=(15,5))\n",
    "        ax[0].set_title(\"Map from Input Data\")\n",
    "        ax[1].set_title(\"Map from Obfuscated Data\")\n",
    "        \n",
    "        grid = torch.empty(npoints,1,2).uniform_(5)-2.5\n",
    "        tr = [grid[:,:,1]**i*grid[:,:,0]**(degree-i) for degree in range(MAP_PARAMS+1) for i in range(degree,-1,-1)]\n",
    "        poly_grid = torch.cat(tr,1)\n",
    "        poly_grid = poly_grid.view(-1,6)\n",
    "        \n",
    "        bx = signal_map_params(x,MAP_PARAMS).float()\n",
    "        by = signal_map_params(y,MAP_PARAMS).float()  \n",
    "        x_predicted_rss = torch.mm(poly_grid,bx)\n",
    "        y_predicted_rss = torch.mm(poly_grid,by)\n",
    "        \n",
    "        ax[0].scatter(grid[:,:,1].numpy(),grid[:,:,0].numpy(),c=x_predicted_rss.numpy())\n",
    "        ax[1].scatter(grid[:,:,1].numpy(),grid[:,:,0].numpy(),c=y_predicted_rss.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "def privatizer_loss(x,y,u,uhat):\n",
    "    bx = signal_map_params(x,MAP_PARAMS)\n",
    "    by = signal_map_params(y,MAP_PARAMS)\n",
    "    l1 = (bx-by).pow(2).mean()\n",
    "    l2 = (x-y).pow(2).mean()\n",
    "    l3 = (y[:,:,[1+3*i for i in range(NUM_POINTS_PER_ENTRY)] + [2+3*i for i in range(NUM_POINTS_PER_ENTRY)]]-\n",
    "          x[:,:,[1+3*i for i in range(NUM_POINTS_PER_ENTRY)] + [2+3*i for i in range(NUM_POINTS_PER_ENTRY)]]).pow(2).mean() # check this power\n",
    "#     cx = density_count(x,NUM_GRIDS)\n",
    "#     cy = density_count(y,NUM_GRIDS)\n",
    "#     l4 = (cx-cy).pow(2).mean()/(BATCH_SIZE*NUM_POINTS_PER_ENTRY)\n",
    "    l = torch.nn.CrossEntropyLoss()\n",
    "    l5 = l(uhat,u)\n",
    "    w1,w2,w3,w4 = UTILITY_WEIGHTS\n",
    "    return RHO*(w1*l1+w2*l2+w3*l3)-(1-RHO)*l5 # TODO replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly(degree, long, lat):\n",
    "    return torch.cat([long**i*lat**(degree-i) for degree in range(MAP_PARAMS+1) for i in range(degree,-1,-1)],1)\n",
    "\n",
    "def signal_map_params(x,degree):    \n",
    "    polynomial = poly(degree, x[:,:,[2+3*i for i in range(NUM_POINTS_PER_ENTRY)]].reshape(-1,1), \n",
    "                      x[:,:,[1+3*i for i in range(NUM_POINTS_PER_ENTRY)]].reshape(-1,1))\n",
    "    beta = torch.mm(torch.inverse(torch.mm(torch.transpose(polynomial,0,1), polynomial)),\n",
    "                  torch.mm(torch.transpose(polynomial,0,1), x[:,:,[0+3*i for i in range(NUM_POINTS_PER_ENTRY)]].reshape(-1,1)))\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def density_count(x, num_grids):\n",
    "#     count = torch.zeros(num_grids,num_grids)\n",
    "#     for i in range(NUM_POINTS_PER_ENTRY):\n",
    "#         count += partial_density_count(x[:,:,i*4:(i+1)*4], num_grids)\n",
    "#     return count\n",
    "    \n",
    "\n",
    "# def partial_density_count(x, num_grids):\n",
    "#     count = torch.zeros(num_grids,num_grids)\n",
    "#     x1min=torch.min(x[:,:,3])\n",
    "#     x2min=torch.min(x[:,:,2])\n",
    "#     size1 = torch.max(x[:,:,3])-x1min\n",
    "#     size2 = torch.max(x[:,:,2])-x2min\n",
    "#     a_all = []\n",
    "#     c_all = []\n",
    "#     for i in range(num_grids):\n",
    "#         for j in range(num_grids):\n",
    "#             a = x1min+(size1/num_grids*i)\n",
    "#             a_all.append(a)\n",
    "#             b = x1min+(size1/num_grids*(i+1))\n",
    "#             a_all.append(b)\n",
    "#             c = x2min+(size2/num_grids*j)\n",
    "#             c_all.append(c)\n",
    "#             d = x2min+(size2/num_grids*(j+1))\n",
    "#             c_all.append(d)\n",
    "#             if i == num_grids-1 and j != num_grids-1:\n",
    "#                 count[i][j] += x[(x[:,:,3] >= a ) & \n",
    "#                                  (x[:,:,3] <= b) & \n",
    "#                                  (x[:,:,2] >= c) & \n",
    "#                                  (x[:,:,2] < d)].size(0)\n",
    "#             elif j == num_grids-1 and i != num_grids-1:\n",
    "#                 count[i][j] += x[(x[:,:,3] >= a ) & \n",
    "#                                  (x[:,:,3] < b) & \n",
    "#                                  (x[:,:,2] >= c) & \n",
    "#                                  (x[:,:,2] <= d)].size(0)\n",
    "#             elif j == num_grids-1 and i == num_grids-1:\n",
    "#                 count[i][j] += x[(x[:,:,3] >= a ) & \n",
    "#                                  (x[:,:,3] <= b) & \n",
    "#                                  (x[:,:,2] >= c) & \n",
    "#                                  (x[:,:,2] <= d)].size(0)\n",
    "#             else:\n",
    "#                 count[i][j] += x[(x[:,:,3] >= a ) & \n",
    "#                                  (x[:,:,3] < b) & \n",
    "#                                  (x[:,:,2] >= c) & \n",
    "#                                  (x[:,:,2] < d)].size(0)\n",
    "#     return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversary_loss(u,uhat):\n",
    "    l = torch.nn.CrossEntropyLoss()\n",
    "    return l(uhat,u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define privatizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "gap_privatizer = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_FEATURES, NUM_UNITS),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(NUM_UNITS, NUM_UNITS),\n",
    "    torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(NUM_UNITS, NUM_UNITS),\n",
    "#     torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(NUM_UNITS, NUM_UNITS),\n",
    "#     torch.nn.ReLU(),\n",
    "    torch.nn.Linear(NUM_UNITS, NUM_FEATURES)\n",
    ")\n",
    "# gap_privatizer.apply(init_weights)\n",
    "gap_privatizer.double()\n",
    "gap_privatizer_optimizer = optim.Adam(gap_privatizer.parameters(),lr=0.001, betas=(0.9,0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_privatizer(x):\n",
    "    normvec = torch.norm(x,p=2,dim=2)\n",
    "    scalevec = NORM_CLIP/normvec\n",
    "    scalevec[scalevec>1] = 1\n",
    "    x = torch.transpose(torch.transpose(x,0,1)*scalevec,0,1).double()\n",
    "    sigma = (NORM_CLIP/EPSILON)*math.sqrt(2*math.log(1.25/DELTA))\n",
    "    noise = torch.normal(mean=torch.zeros_like(x),std=sigma).double()\n",
    "    y = x + noise\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise_privatizer(x):\n",
    "    noise = torch.normal(mean=torch.zeros_like(x),std=SIGMA).double()\n",
    "    y = x + noise\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define adversary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "512, 128, 0.01, 33%  \n",
    "256, 128, 32, 43% no bias  \n",
    "256, 128, 32, 46% bias 0.1  \n",
    "256, 128, 32, 53% bias 0.25, 0.1  \n",
    "256, 128, 32, 56% bias 0.25, 0.1, lr 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == torch.nn.Linear:\n",
    "        torch.nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        torch.nn.init.normal_(m.bias, mean=0.25, std=0.1)\n",
    "#         m.bias.data.fill_(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=450, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=32, out_features=9, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adversary = torch.nn.Sequential(\n",
    "    torch.nn.Linear(NUM_FEATURES, 128),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, 32),\n",
    "    torch.nn.ReLU(),\n",
    "#     torch.nn.Linear(NUM_UNITS, NUM_UNITS),\n",
    "#     torch.nn.ReLU(),\n",
    "    torch.nn.Linear(32, NUM_USERS)\n",
    ")\n",
    "adversary.apply(init_weights)\n",
    "adversary.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversary_optimizer = optim.Adam(adversary.parameters(),lr=0.001, betas=(0.9,0.999))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRIVATIZER = noise_privatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# for each epoch\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    \n",
    "    # iterate through the training dataset\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        \n",
    "        # unpack batch\n",
    "        x, u = batch['x'], batch['u'].squeeze()\n",
    "        \n",
    "        if PRIVATIZER == gap_privatizer:\n",
    "            # reset privatizer gradients\n",
    "            gap_privatizer_optimizer.zero_grad()\n",
    "        \n",
    "        # privatize x\n",
    "        y = PRIVATIZER(x)\n",
    "        \n",
    "        # reset adversary gradients\n",
    "        adversary_optimizer.zero_grad()\n",
    "        \n",
    "        # estimate userIDs and original locations\n",
    "        uhat = adversary(y).squeeze()\n",
    "        \n",
    "        # train adversary\n",
    "        aloss = adversary_loss(u,uhat)\n",
    "        aloss.backward(retain_graph=True)\n",
    "        torch.nn.utils.clip_grad_norm_(adversary.parameters(), 1000)\n",
    "        adversary_optimizer.step()\n",
    "        \n",
    "        # evaluate utility loss\n",
    "        ploss = privatizer_loss(x,y,u,uhat)\n",
    "        \n",
    "        if PRIVATIZER == gap_privatizer:\n",
    "            # train privatizer\n",
    "            ploss.backward()\n",
    "            # check that gradients are non-zero\n",
    "            # for param in gap_privatizer.parameters():\n",
    "                # print(param.grad)\n",
    "            torch.nn.utils.clip_grad_norm_(gap_privatizer.parameters(), 1000)\n",
    "            gap_privatizer_optimizer.step()\n",
    "        \n",
    "        # print progress\n",
    "        if i % 30 == 29:\n",
    "            pass\n",
    "#             print(i+1,\"aloss:\",aloss.item(),\"ploss:\",ploss.item())\n",
    "          \n",
    "        # stop early\n",
    "        if i == 0:\n",
    "            pass\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIGMA =  0.75\n",
      "Adversary Accuracy: 40.625\n",
      "Adversary Loss: 1.5272658922194675\n",
      "Sigmal Map: 0.01795038650262015\n",
      "Distortion: 0.569884939918935\n",
      "Geographic Distortion: 0.5640739124094594\n"
     ]
    }
   ],
   "source": [
    "# do not keep track of gradients\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    dist_error = 0\n",
    "    l1,l2,l3,l4,l5 = 0,0,0,0,0\n",
    "    \n",
    "    # iterate through test data\n",
    "    for i,batch in enumerate(test_loader):\n",
    "        \n",
    "        # unpack batch\n",
    "        x, u = batch['x'], batch['u'].squeeze()\n",
    "        \n",
    "        # privatize x\n",
    "        y = PRIVATIZER(x)\n",
    "        \n",
    "        # estimate userID and location\n",
    "        estimate = adversary(x).squeeze()\n",
    "        uhat, lochat = estimate[:,:9], estimate[:,9:]\n",
    "        \n",
    "        # Privacy Metric\n",
    "        _, upred = torch.max(uhat.data,1)\n",
    "        total+=u.size(0)\n",
    "        correct+=(upred==u).sum().item()\n",
    "#         print(\"u:\", u)\n",
    "#         print(\"p:\",upred)\n",
    "#         print(uhat.data[0])\n",
    "#         dist_error += (x[:,:,12:14]-lochat).pow(2).mean().item()\n",
    "        \n",
    "        # Utility Metrics       \n",
    "        bx = signal_map_params(x,MAP_PARAMS)\n",
    "        by = signal_map_params(y,MAP_PARAMS)\n",
    "        l1 = (bx-by).pow(2).mean()\n",
    "        l2 = (x-y).pow(2).mean()\n",
    "        l3 = (y[:,:,[1+3*i for i in range(NUM_POINTS_PER_ENTRY)] + [2+3*i for i in range(NUM_POINTS_PER_ENTRY)]]-\n",
    "              x[:,:,[1+3*i for i in range(NUM_POINTS_PER_ENTRY)] + [2+3*i for i in range(NUM_POINTS_PER_ENTRY)]]).pow(2).mean() # check this power\n",
    "#         cx = density_count(x,NUM_GRIDS)\n",
    "#         cy = density_count(y,NUM_GRIDS)\n",
    "#         l4 = (cx-cy).pow(2).mean()/(BATCH_SIZE*NUM_POINTS_PER_ENTRY)\n",
    "        l = torch.nn.CrossEntropyLoss()\n",
    "        l5 = l(uhat,u)\n",
    "        \n",
    "        # stop early\n",
    "        if i==0:\n",
    "            break\n",
    "        \n",
    "    print(\"SIGMA = \",SIGMA)\n",
    "    print(\"Adversary Accuracy:\", 100*correct/total)\n",
    "#     print(\"Average estimated location error:\",dist_error/(i+1))\n",
    "    \n",
    "    print(\"Adversary Loss:\", l5.item()/(i+1))\n",
    "    print(\"Sigmal Map:\", l1.item()/(i+1))\n",
    "    print(\"Distortion:\", l2.item()/(i+1))\n",
    "    print(\"Geographic Distortion:\", l3.item()/(i+1))\n",
    "#     print(\"Density Count:\",l4.item()/(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_privatizer(x,y,u,uhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# show_maps(x,y,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
